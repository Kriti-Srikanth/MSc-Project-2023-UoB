{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMlNyYi7SEhKu1ApjtrG+Gj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tG1BMJx-226Q"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, concatenate, BatchNormalization, Activation, Dropout\n",
        "\n",
        "def conv_block(tensor, n_filters, kernel_size=3, batchnorm=True):\n",
        "\n",
        "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), padding=\"same\")(tensor)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    x = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), padding=\"same\")(x)\n",
        "    if batchnorm:\n",
        "        x = BatchNormalization()(x)\n",
        "    x = Activation(\"relu\")(x)\n",
        "\n",
        "    return x\n",
        "\n",
        "def deconv_block(tensor, residual, n_filters, kernel_size=3, strides=2, dropout=0.5):\n",
        "    y = UpSampling2D(size=(strides, strides))(tensor)\n",
        "    y = Conv2D(filters=n_filters, kernel_size=(kernel_size, kernel_size), padding=\"same\")(y)\n",
        "    y = concatenate([y, residual], axis=3)\n",
        "    y = Dropout(dropout)(y)\n",
        "    y = conv_block(y, n_filters, kernel_size=3, batchnorm=True)\n",
        "\n",
        "    return y\n",
        "\n",
        "def unet_model(input_shape, n_filters=64, dropout=0.5, batchnorm=True):\n",
        "\n",
        "    input_layer = Input(shape=input_shape)\n",
        "\n",
        "    c1 = conv_block(input_layer, n_filters=n_filters*1, kernel_size=3, batchnorm=batchnorm)\n",
        "    p1 = MaxPooling2D(pool_size=(2, 2))(c1)\n",
        "    p1 = Dropout(dropout)(p1)\n",
        "\n",
        "    c2 = conv_block(p1, n_filters=n_filters*2, kernel_size=3, batchnorm=batchnorm)\n",
        "    p2 = MaxPooling2D(pool_size=(2, 2))(c2)\n",
        "    p2 = Dropout(dropout)(p2)\n",
        "\n",
        "    c3 = conv_block(p2, n_filters=n_filters*4, kernel_size=3, batchnorm=batchnorm)\n",
        "    p3 = MaxPooling2D(pool_size=(2, 2))(c3)\n",
        "    p3 = Dropout(dropout)(p3)\n",
        "\n",
        "    c4 = conv_block(p3, n_filters=n_filters*8, kernel_size=3, batchnorm=batchnorm)\n",
        "    p4 = MaxPooling2D(pool_size=(2, 2))(c4)\n",
        "    p4 = Dropout(dropout)(p4)\n",
        "\n",
        "    c5 = conv_block(p4, n_filters=n_filters*16, kernel_size=3, batchnorm=batchnorm)\n",
        "\n",
        "    u6 = deconv_block(c5, residual=c4, n_filters=n_filters*8, kernel_size=3, strides=2, dropout=dropout)\n",
        "    u7 = deconv_block(u6, residual=c3, n_filters=n_filters*4, kernel_size=3, strides=2, dropout=dropout)\n",
        "    u8 = deconv_block(u7, residual=c2, n_filters=n_filters*2, kernel_size=3, strides=2, dropout=dropout)\n",
        "    u9 = deconv_block(u8, residual=c1, n_filters=n_filters*1, kernel_size=3, strides=2, dropout=dropout)\n",
        "\n",
        "    outputs = Conv2D(1, (1, 1), activation='sigmoid')(u9)\n",
        "\n",
        "    model = Model(inputs=[input_layer], outputs=[outputs])\n",
        "\n",
        "    return model\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "from keras.models import Model\n",
        "from keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Dropout, Flatten, Activation, concatenate, ZeroPadding2D, BatchNormalization\n",
        "from keras.optimizers import Adam\n",
        "from keras.utils import to_categorical\n",
        "import cv2\n",
        "from tqdm import tqdm\n",
        "\n",
        "batch_size = 32\n",
        "epochs = 100\n",
        "\n",
        "PREDICTION_SEGMENTATION_DIR = \"prediction_segmentation\"\n",
        "\n",
        "if not os.path.exists(PREDICTION_SEGMENTATION_DIR):\n",
        "    os.makedirs(PREDICTION_SEGMENTATION_DIR)\n",
        "\n",
        "def read_lines(flist_name, shuffle=True):\n",
        "\n",
        "    with open(flist_name, 'r') as f:\n",
        "        lines = f.readlines()\n",
        "        if shuffle:\n",
        "            np.random.shuffle(lines)\n",
        "    return lines\n",
        "\n",
        "def save_segmentation_masks(model, directory):\n",
        "    all_files = os.listdir(directory)\n",
        "    for file in tqdm(all_files):\n",
        "        img = cv2.imread(os.path.join(directory, file))\n",
        "        img = cv2.resize(img, (INPUT_SIZE[0], INPUT_SIZE[1]))\n",
        "        pred = model.predict(np.expand_dims(img, axis=0))\n",
        "        mask = np.argmax(pred, axis=-1)\n",
        "        mask = np.squeeze(mask, axis=0)\n",
        "        cv2.imwrite(os.path.join(PREDICTION_SEGMENTATION_DIR, file), mask*255)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    for fold_no in range(0, 5):\n",
        "        model = network_unet(input_shape=(INPUT_SIZE[0], INPUT_SIZE[1], 3))\n",
        "        optimizer_name = Adam(lr=1e-4)\n",
        "        model.compile(loss='categorical_crossentropy', optimizer=optimizer_name, metrics=['accuracy'])\n",
        "\n",
        "        for ep in range(epochs):\n",
        "          model.fit(X_train, y_train, batch_size=32, validation_split=0.1)\n",
        "          model_path = os.path.join(root_dir, f'u_net_fold_{fold_no}.h5')\n",
        "          model.save(model_path)\n",
        "\n",
        "\n",
        "    test_directory = \"data_kaggle/test\"\n",
        "    save_segmentation_masks(model, test_directory)\n"
      ],
      "metadata": {
        "id": "EOH_syKK40tQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Test 1\n",
        "\n"
      ],
      "metadata": {
        "id": "drrR5puwRSUY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G7FSxQpTRVmD"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}